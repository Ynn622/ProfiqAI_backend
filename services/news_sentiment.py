import pandas as pd
import time
from transformers import BertTokenizer, BertForSequenceClassification
import torch
import gc

from util.config import Env  # ç¢ºä¿ç’°å¢ƒè®Šæ•¸è¢«è¼‰å…¥
from util.data_manager import DataManager
from services.news_data import get_udn_news_summary, parse_article

# æ¨¡å‹ (HuggingFace è·¯å¾‘)
model_name = "Ynn22/news_model"
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertForSequenceClassification.from_pretrained(model_name)

@torch.no_grad()
def predict_sentiment(text: str) -> torch.Tensor:
    """
    é æ¸¬æ–°èçš„æƒ…æ„Ÿåˆ†æ•¸
    Args:
        text (str): æ–°èæ–‡æœ¬
    Returns:
        torch.Tensor: ä¸‰åˆ†é¡çš„æ©Ÿç‡åˆ†æ•¸ (æ­£å‘, ä¸­ç«‹, è² å‘)
    """
    tokens = tokenizer(text, return_tensors="pt", truncation=False, padding=False)
    input_ids = tokens['input_ids'][0]
    num_tokens = len(input_ids)  #æ–‡æœ¬tokensæ•¸é‡
    max_length = 512

    parts = []
    if num_tokens <= max_length:  #å°æ–¼tokenä¸ç”¨åˆ†å‰²ï¼Œå¤§æ–¼tokenè¦
        parts.append(text)
        num_parts = 1
    else:
        num_parts = (num_tokens + max_length - 1) // max_length  #åˆ‡æˆå¹¾æ®µ
        part_word = len(text) // num_parts  #æ¯ä¸€æ®µçš„å­—æ•¸
        parts = [text[i * part_word : (i + 1) * part_word] for i in range(num_parts)]
        parts[-1] = text[(num_parts - 1) * part_word:]  #æœ€å¾Œä¸€æ®µ

    total_probs = torch.zeros(3, device=model.device)

    for part in parts:
        inputs = tokenizer(part, return_tensors="pt", truncation=True, padding=True, max_length=max_length).to(model.device)
        outputs = model(**inputs)
        probs = torch.nn.functional.softmax(outputs.logits, dim=-1).squeeze()
        total_probs += probs

    avg_probs = total_probs / num_parts
    return avg_probs


def cal_news_sentiment(stock_id: str, page: int=1) -> pd.DataFrame:
    """
    è¨ˆç®—å€‹è‚¡çš„æ–°èæƒ…æ„Ÿåˆ†æ•¸
    """
    news_summary_df = get_udn_news_summary(stock_id, page=page).iloc[:10]
    
    scores = []
    contents = []
    for i in range(len(news_summary_df)):
        url = news_summary_df['Url'].iloc[i]
        source = news_summary_df['Source'].iloc[i]

        cached = None
        if url:
            cached = DataManager.get_score(
                stock_id=None,
                score_type="news",
                table_name=DataManager.NEWS_TABLE,
                key_fields={"url": url},
            )
        if cached:
            cached_data = cached.get("data", cached)
            cached_score = [
                cached_data.get("positive"),
                cached_data.get("neutral"),
                cached_data.get("negative"),
            ]
            cached_content = cached_data.get("content")
            if cached_content and all(v is not None for v in cached_score):
                scores.append(cached_score)
                contents.append(cached_content)
                continue

        if Env.RELOAD: print(f"è¨ˆç®—æƒ…æ„Ÿåˆ†æ•¸ä¸­ï¼š{i+1}/{len(news_summary_df)}   ", end="\r")
        text = parse_article(url, source=source)

        try:
            score = predict_sentiment(text)
            score_list = score.cpu().tolist()  # å¾GPUæ¬å›CPUï¼Œé¿å…å †ç©
            scores.append(score_list)
            contents.append(text)

            if url:
                DataManager.save_score(
                    stock_id=None,
                    data={
                        "positive": score_list[0],
                        "neutral": score_list[1],
                        "negative": score_list[2],
                        "content": text,
                    },
                    score_type="news",
                    table_name=DataManager.NEWS_TABLE,
                    key_fields={"url": url},
                    conflict_keys=("url",),
                    flatten_data=True,
                    include_data=False,
                )
        except Exception as e:
            print(f"ğŸ”´ [Error] At {i}: {e}")
            scores.append([None, None, None])
            contents.append(None)
        torch.cuda.empty_cache()  # æ¸…ç†è¨˜æ†¶é«”
        gc.collect()
    score_df = pd.DataFrame(scores, columns=['positive', 'neutral', 'negative'])
    score_df["content"] = contents
    score_df.index = news_summary_df.index
    score_df.index.name = "æ—¥æœŸ"
    if Env.RELOAD: print(f"\r Done: æ–°èæƒ…æ„Ÿåˆ†æ•¸è¨ˆç®—å®Œç•¢ï¼{' '*40}")
    data = pd.concat([news_summary_df, score_df], axis=1)[['Url', 'content', 'positive', 'neutral', 'negative']]
    return data

def classify_sentiment(pos, neu, neg):
    """ä¾å¹³å‡åˆ†æ•¸åˆ¤æ–·æ•´é«”æƒ…ç·’"""
    if pos > 0.5:
        return "æ¥µæ­£", 2
    if neg > 0.5:
        return "æ¥µè² ", -2
    if neu > 0.5:
        return "ä¸­ç«‹", 0

    # ç„¡é¡åˆ¥ >0.5 â†’ å–æœ€å¤§è€…
    max_cat = max([("positive", pos), ("neutral", neu), ("negative", neg)], key=lambda x: x[1])[0]
    return {
        "positive": ("åå¤š", 1),
        "negative": ("åç©º", -1),
        "neutral": ("ä¸­ç«‹", 0)
    }[max_cat]


def total_news_sentiment(stock_id: str, page: int = 1) -> dict:
    """
    è¨ˆç®—å€‹è‚¡çš„ç¸½é«”æ–°èæƒ…æ„Ÿåˆ†æ•¸
    """
    sentiment_df = cal_news_sentiment(stock_id, page=page)
    contents = sentiment_df['content'].dropna().tolist() if not sentiment_df.empty else []

    if sentiment_df.empty:
        return {
            "direction_label": "ç„¡è³‡æ–™",
            "direction": 0,
            "positive": 0,
            "neutral": 0,
            "negative": 0,
            "contents": []
        }

    avg_pos = sentiment_df['positive'].mean()
    avg_neu = sentiment_df['neutral'].mean()
    avg_neg = sentiment_df['negative'].mean()
    overall, score = classify_sentiment(avg_pos, avg_neu, avg_neg)

    return {
        "direction": score,
        "direction_label": overall,
        "positive": avg_pos,
        "neutral": avg_neu,
        "negative": avg_neg,
        "contents": contents,
    }
