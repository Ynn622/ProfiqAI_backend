import pandas as pd
from stockstats import StockDataFrame as Sdf
import cloudscraper
import requests
from bs4 import BeautifulSoup as bs
from datetime import datetime, timedelta
import yfinance as yf
import numpy as np
import re
import html
import time
from util.logger import printf, Color

def fetchStockInfo(stockName: str) -> str:
    """
    è‚¡ç¥¨ä»£è™Ÿ&åç¨±æŸ¥è©¢ã€‚
    toolFetchStockInfo() æœƒè‡ªå‹•èª¿ç”¨æ­¤å‡½æ•¸ã€‚
    """
    try:
        url = f"https://tw.stock.yahoo.com/_td-stock/api/resource/WaferAutocompleteService;view=wafer&query={stockName}"
        response = requests.get(url)
        stockID = bs(response.json()["html"], features="lxml").find("a")["href"].split('stock_id=')[1]
        stockName = bs(response.json()["html"], features="lxml").find("span").text
    except Exception as e:
        stockID = None
        stockName = None
    return stockID, stockName


def getStockPrice(symbol: str, start: str, sdf_indicator_list: list[str]=[] ) -> pd.DataFrame:
    """
    å–å¾—æŒ‡å®šè‚¡ç¥¨çš„æ­·å²è‚¡åƒ¹è³‡æ–™ã€‚
    toolGetStockPrice() æœƒè‡ªå‹•èª¿ç”¨æ­¤å‡½æ•¸ã€‚
    """
    data = yf.Ticker(symbol).history(period="2y").round(2)
    del data["Dividends"], data["Stock Splits"]
    if "Capital Gains" in data.columns: del data["Capital Gains"]
    data.index = data.index.strftime("%Y-%m-%d")
    data["Volume"] = data["Volume"]*0.001  # å°‡æˆäº¤é‡è½‰æ›ç‚ºå¼µæ•¸
    
    # çˆ¬å–ç¾åœ¨å³æ™‚è‚¡åƒ¹è³‡æ–™
    try:
        live_df = get_live_price(symbol)
        data = data.drop(live_df.index[0], errors='ignore') 
        data = pd.concat([data, live_df])
    except Exception as e:
        printf(f"ğŸ”´ [Error] çˆ¬å–å³æ™‚è‚¡åƒ¹è³‡æ–™éŒ¯èª¤: {str(e)}", color=Color.RED)
    
    # æŒ‡æ¨™è¨ˆç®—
    if sdf_indicator_list:
        indicator_df = get_technical_indicators(data, sdf_indicator_list)
        try:
            data = pd.concat([data, indicator_df], axis=1)
        except Exception as e:
            printf(f"ğŸ”´ [Error] æŒ‡æ¨™è¨ˆç®—éŒ¯èª¤: {str(e)}", color=Color.RED)

    data = data[data.index >= start]  # ç¢ºä¿è³‡æ–™å¾æŒ‡å®šæ—¥æœŸé–‹å§‹
    data = data.dropna().round(2)  # ç§»é™¤åŒ…å«NaNçš„è¡Œ 
    
    # ç±Œç¢¼é¢è³‡æ–™
    if symbol not in ("^TWII", "^TWOII"):
        try:
            chip_data = get_chip_data(symbol, data.index[0], data.index[-1]).reindex(data.index)
            data = pd.concat([data, chip_data], axis=1)
        except Exception as e:
            printf(f"ğŸ”´ [Error] ç±Œç¢¼é¢è³‡æ–™éŒ¯èª¤: {str(e)}", color=Color.RED)

    return data


def FetchStockNews(stock_name: str) -> pd.DataFrame:
    """
    çˆ¬å–æŒ‡å®šè‚¡ç¥¨çš„æœ€æ–°æ–°èè³‡æ–™ã€‚
    toolFetchStockNews() æœƒè‡ªå‹•èª¿ç”¨æ­¤å‡½æ•¸ã€‚
    """
    data = []
    col = ["Date","Title","Content"]
    url = f"https://ess.api.cnyes.com/ess/api/v1/news/keyword?q={stock_name}&limit=10&page=1"
    json_news = requests.get(url).json()['data']['items']
    for item in json_news:
        id = item['newsId']
        title = item['title']
        title = re.sub(r'<.*?>', '', title)
        if "ç›¤ä¸­é€Ÿå ±" in title:continue
        t = item['publishAt']+28800
        if time.mktime(time.gmtime())-2592000>t:continue
        news_time = time.strftime("%Y/%m/%d", time.gmtime(t))
        news_url = f"https://news.cnyes.com/news/id/{id}"
        news = requests.get(news_url).text
        news_bs = bs(news,'html.parser')
        news_find = news_bs.find("main",class_="c1tt5pk2")
        news_data = "\n".join(x.text.strip() for x in news_find)
        news_data = news_data.replace("ã€€ã€€ã€€","").replace("\n\n","")
        delete_strings = ["æ­¡è¿å…è²»è¨‚é–±", "ç²¾å½©å½±ç‰‡","ç²‰çµ²åœ˜", "Line ID","Line@","ä¾†æºï¼š"]
        for delete_str in delete_strings:
            index = news_data.find(delete_str)
            if index != -1:
                news_data = news_data[:index]  # åªä¿ç•™ä¸åŒ…å«è©²å­—ä¸²çš„éƒ¨åˆ†
                break
        data.append([news_time, title, news_data])
    return pd.DataFrame(data, columns=col)

def FetchTwiiNews() -> pd.DataFrame:
    """
    çˆ¬å–å°ç£åŠ æ¬ŠæŒ‡æ•¸(^TWII)èˆ‡æ«ƒè²·å¸‚å ´(^TWOII)çš„æœ€æ–°æ–°èã€‚
    toolFetchTwiiNews() æœƒè‡ªå‹•èª¿ç”¨æ­¤å‡½æ•¸ã€‚
    """
    data = []
    col = ["Date","Title","Content"]
    start = datetime.now()
    end = start - timedelta(days=1)
    start = end - timedelta(days=20)
    url = f"https://api.cnyes.com/media/api/v1/newslist/category/tw_quo?page=1&limit=15&startAt={int(start.timestamp())}&endAt={int(end.timestamp())}"
    web = requests.get(url).json()['items']
    json_news = web['data']
    for i in range(web['to']-web['from']+1):
        content = json_news[i]["content"]
        content = re.sub(r'<.*?>', '', html.unescape(content))
        if content.find("http")!=-1:
            content = content[:content.find("http")]
        title = json_news[i]["title"]
        title = re.sub(r"^ã€ˆ.*?ã€‰", "", title)
        timestamp = json_news[i]["publishAt"]+28800
        news_time = time.strftime("%Y/%m/%d %H:%M", time.gmtime(timestamp))
        data.append([news_time,title,content])
    return pd.DataFrame(data,columns=col)

def fetchETFIngredients(ETF_name: str) -> str:
    """
    æŸ¥è©¢ ETF çš„æˆåˆ†è‚¡ã€‚
    toolFetchETFIngredients() æœƒè‡ªå‹•èª¿ç”¨æ­¤å‡½æ•¸ã€‚
    """
    url = f"https://tw.stock.yahoo.com/quote/{ETF_name}/holding"
    response = requests.get(url)
    soup = bs(response.text, "html.parser")
    table = soup.find_all("ul", class_="Bxz(bb) Bgc($c-light-gray) Bdrs(8px) P(20px)")[1].find_all("li")[1:]
    data = ""
    for i in table:
        data += i.text.strip() + "\n"
    return data

''' ä¸‹æ–¹ç‚ºè¼”åŠ©å‡½æ•¸ '''

def get_technical_indicators(data, sdf_indicator_list):
    """
    è¨ˆç®—æŠ€è¡“æŒ‡æ¨™
    Args:
        data(DataFrame): è‚¡åƒ¹æ­·å²è³‡æ–™
        sdf_indicator_list (list): æ¬²è¨ˆç®—çš„æŠ€è¡“æŒ‡æ¨™æ¸…å–®
    """
    indicator_dict = {
        'close_5_sma':'SMA_5',
        'close_10_sma':'SMA_10',
        'close_20_sma':'SMA_20',
        'close_60_sma':'SMA_60',
        'close_5_ema':'EMA_5',
        'close_10_ema':'EMA_10',
        'close_20_ema':'EMA_20',
        'macd': 'MACD',
        'macds': 'Signal Line',
        'macdh': 'Histogram',
        'kdjk': '%K',
        'kdjd': '%D',
        'rsi_5': 'RSI_5',
        'rsi_10': 'RSI_10',
        'close_5_roc': 'ROC',
        'boll_ub': 'BOLL_UPPER',
        'boll': 'BOLL_MIDDLE',
        'boll_lb': 'BOLL_LOWER',
        'change': 'PCT'
    }

    # è¨ˆç®—æŠ€è¡“æŒ‡æ¨™
    stock_df = Sdf.retype(data)
    indicator_data = stock_df[sdf_indicator_list].copy()
    indicator_data.rename(columns=indicator_dict, inplace=True)  # å°‡æŒ‡æ¨™åç¨±è½‰æ›
    indicator_data = indicator_data.round(2)
    
    return indicator_data


def get_live_price(symbol: str) -> pd.DataFrame:
    """
    ç”¨æ–¼å–å¾—æœ€æ–°å³æ™‚è‚¡åƒ¹è³‡æ–™ã€‚
    get_stock_price() æœƒè‡ªå‹•èª¿ç”¨æ­¤å‡½æ•¸ã€‚
    """
    header = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36"}
    url = f"https://tw.stock.yahoo.com/quote/{symbol}"
    web = requests.get(url,headers=header,timeout=5)
    bs_web = bs(web.text,"html.parser")
    table = bs_web.find("ul",class_="D(f) Fld(c) Flw(w) H(192px) Mx(-16px)").find_all("li")
    name = ["Close","Open","High","Low","Volume"]
    dic = {}
    s_list = [0,1,2,3,5 if symbol in ("^TWII", "^TWOII") else 9]  # å¤§ç›¤&æ«ƒè²· æŠ“å–æ¬„ä½ä¸åŒ
    for i in range(5):
        search = s_list[i]
        row = float(table[search].find_all("span")[1].text.replace(",",""))
        dic[name[i]]=[row]
    nowtime = bs_web.find("time").find_all("span")[2].text
    nowtime = pd.to_datetime(nowtime).strftime("%Y-%m-%d")
    return pd.DataFrame(dic, index=[nowtime])

 
def get_chip_data(symbol: str, start: str, end: str) -> pd.DataFrame:
    """
    ç”¨æ–¼å–å¾—æœ€æ–°ç±Œç¢¼é¢è³‡æ–™ã€‚
    get_stock_price() æœƒè‡ªå‹•èª¿ç”¨æ­¤å‡½æ•¸ã€‚
    """
    if symbol in ("^TWII", "^TWOII"):
        printf(f"[function] get_chip_data(): ä¸æä¾›ç±Œç¢¼é¢è³‡æ–™: {symbol}", color=Color.PURPLE)
        return pd.DataFrame()
    symbol = symbol.split(".")[0]  # å»é™¤å¾Œç¶´
    url = f"https://fubon-ebrokerdj.fbs.com.tw/z/zc/zcl/zcl.djhtm?a={symbol}&c={start}&d={end}"
    scraper = cloudscraper.create_scraper()  # ä½¿ç”¨ cloudscraper çˆ¬å–
    web = scraper.get(url).text
    bs_table = bs(web, "html.parser").find("table", class_="t01").find_all("tr")[7:-1]  # è·³éå‰7è¡Œå’Œæœ€å¾Œä¸€è¡Œ
    col = ["å¤–è³‡", "æŠ•ä¿¡", "è‡ªç‡Ÿå•†", "ä¸‰å¤§æ³•äººåˆè¨ˆ"]
    data = []
    date_index = []
    for i in bs_table[::-1]: # åå‘éæ­·ï¼Œå› ç‚ºæœ€æ–°çš„è³‡æ–™åœ¨æœ€å¾Œä¸€è¡Œ
        tds = i.find_all("td")[:5]
        date = tds.pop(0).text.split('/')   # å–å‡ºæ—¥æœŸ

        texts = [td.text.strip() for td in tds]
        # åµæ¸¬æ˜¯å¦æœ‰ '--'
        if any(text == '--' for text in texts):
            continue  # ä¸ç¹¼çºŒè™•ç†é€™ç­†è³‡æ–™
        # æ­£å¸¸è™•ç†æ•¸å­—
        row = [int(text.replace(",", "")) for text in texts]
        data.append(row)
        # è™•ç†æ—¥æœŸ
        date_str = f"{int(date[0])+1911}-{date[1]}-{date[2]}"
        date_index.append(date_str)
    df = pd.DataFrame(data, columns=col, index=date_index)
    return df


def get_live_stock_info(stockID: str) -> dict:
    """
    ç”¨æ–¼å–å¾—æœ€æ–°å³æ™‚è‚¡åƒ¹è³‡æ–™èˆ‡ç›¸é—œè³‡è¨Šã€‚
    """
    quoteWeb = requests.get(f'https://tw.stock.yahoo.com/quote/{stockID}')
    soup = bs(quoteWeb.text, "html.parser")

    info = {}

    nowtime = soup.find("time").find_all("span")[2].text
    nowtime = pd.to_datetime(nowtime).strftime("%Y-%m-%d")
    info['Date'] = nowtime
    info['StockName'] = soup.find('h1', class_='C($c-link-text) Fw(b) Fz(24px) Mend(8px)').text

    priceTable = soup.find("ul",class_="D(f) Fld(c) Flw(w) H(192px) Mx(-16px)").find_all("li")
    dic = {'Close': 0,
        'Open': 1,
        'High': 2,
        'Low': 3,
        'Volume': 5 if stockID in ("^TWII", "^TWOII") else 9,
        'Change': 8,
        'ChangePct': 7,
        'PreClose': 6}

    for key, value in dic.items():
        row = float(priceTable[value].find_all("span")[1].text.replace(",","").replace("%",""))
        dic[key] = row

    info.update(dic)

    info['Trend'] = True if info['PreClose']<info['Close'] else False
    return info
